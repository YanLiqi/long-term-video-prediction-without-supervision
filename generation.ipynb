{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrR8cKOY-RHw"
   },
   "source": [
    "## Inference on pretrained models.\n",
    "\n",
    "[Install Jupyter](http://jupyter.org/install) to open this file.\n",
    "\n",
    "You can use this to run inference on pretrained models saved to Google Cloud, or\n",
    "modify it to do inference on your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "di5a4shsA5At"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import prediction_input\n",
    "import prediction_model\n",
    "\n",
    "def save_png(image_array, path):\n",
    "  \"\"\"Saves an image to disk.\n",
    "\n",
    "  Args:\n",
    "    image_array: numpy array of shape [image_size, image_size, 3].\n",
    "    path: str, output file.\n",
    "  \"\"\"\n",
    "  buf = io.BytesIO()\n",
    "  scipy.misc.imsave(buf, image_array, format='png')\n",
    "  buf.seek(0)\n",
    "  f = tf.gfile.GFile(path, 'w')\n",
    "  f.write(buf.getvalue())\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YnVAnfVGylyP"
   },
   "outputs": [],
   "source": [
    "COLOR_CHAN = 3\n",
    "IMG_WIDTH = 64\n",
    "IMG_HEIGHT = 64\n",
    "\n",
    "IMAGE_FEATURE_NAME = \"images\"\n",
    "JOINT_POSE_FEATURE_NAME = \"joint_poses\"\n",
    "ACTION_FEATURE_NAME = \"actions\"\n",
    "\n",
    "def get_input_fn_queue_determ(pattern, batch_size, flags):\n",
    "  def input_fn(params=None):\n",
    "    \"\"\"Input function using queues for GPU, always returns examples in the same order.\"\"\"\n",
    "    del params\n",
    "    filenames = gfile.Glob(os.path.join(flags.data_dir, pattern))\n",
    "    if not filenames:\n",
    "      raise RuntimeError('No data files found.')\n",
    "    filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "    reader = tf.TFRecordReader()\n",
    "\n",
    "    _, val = reader.read(filename_queue)\n",
    "    serialized_input = tf.reshape(val, shape=[1])\n",
    "\n",
    "    image_seq = None\n",
    "\n",
    "    for i in range(0, flags.sequence_length, flags.skip_num):\n",
    "      image_name = 'image_' + str(i)\n",
    "\n",
    "      if flags.dataset_type == 'robot':\n",
    "        pose_name = 'state_' + str(i)\n",
    "        action_name = 'action_' + str(i)\n",
    "        joint_pos_name = 'joint_positions_' + str(i)\n",
    "        features = {\n",
    "            pose_name:\n",
    "                tf.FixedLenFeature([flags.pose_dim], tf.float32),\n",
    "            image_name:\n",
    "                tf.FixedLenFeature([1], tf.string),\n",
    "            action_name:\n",
    "                tf.FixedLenFeature([flags.pose_dim], tf.float32),\n",
    "            joint_pos_name:\n",
    "                tf.FixedLenFeature([flags.joint_pos_dim], tf.float32)\n",
    "        }\n",
    "      else:\n",
    "        features = {\n",
    "            image_name: tf.FixedLenFeature([1], tf.string),\n",
    "        }\n",
    "\n",
    "      parsed_input = tf.parse_example(serialized_input, features)\n",
    "\n",
    "      # Process image\n",
    "      image_buffer = tf.reshape(parsed_input[image_name], shape=[])\n",
    "      image = tf.image.decode_jpeg(image_buffer, channels=COLOR_CHAN)\n",
    "      image = tf.image.resize_images(\n",
    "          image, (IMG_HEIGHT, IMG_WIDTH), method=tf.image.ResizeMethod.BICUBIC)\n",
    "      image = tf.cast(tf.expand_dims(image, 0), tf.float32) / 255.0\n",
    "\n",
    "      if flags.dataset_type == 'robot':\n",
    "        pose = tf.reshape(parsed_input[pose_name], shape=[flags.pose_dim])\n",
    "        pose = tf.expand_dims(pose, 0)\n",
    "        action = tf.reshape(parsed_input[action_name], shape=[flags.pose_dim])\n",
    "        action = tf.expand_dims(action, 0)\n",
    "        joint_pos = tf.reshape(\n",
    "            parsed_input[joint_pos_name], shape=[flags.joint_pos_dim])\n",
    "        joint_pos = tf.expand_dims(joint_pos, 0)\n",
    "      else:\n",
    "        pose = tf.zeros([1, flags.pose_dim])\n",
    "        action = tf.zeros([1, flags.pose_dim])\n",
    "        joint_pos = tf.zeros([1, flags.joint_pos_dim])\n",
    "\n",
    "      if i == 0:\n",
    "        image_seq = image\n",
    "        action_seq, pose_seq, joint_pos_seq = action, pose, joint_pos\n",
    "      else:\n",
    "        image_seq = tf.concat([image_seq, image], 0)\n",
    "        action_seq = tf.concat([action_seq, action], 0)\n",
    "        pose_seq = tf.concat([pose_seq, pose], 0)\n",
    "        joint_pos_seq = tf.concat([joint_pos_seq, joint_pos], 0)\n",
    "\n",
    "    [images, actions, poses, joint_pos] = tf.train.batch(\n",
    "        [image_seq, action_seq, pose_seq, joint_pos_seq],\n",
    "        batch_size,\n",
    "        enqueue_many=False,\n",
    "        capacity=100 * batch_size)\n",
    "\n",
    "    print(flags.sequence_length)\n",
    "    joint_poses = tf.concat([joint_pos, poses], 2)\n",
    "\n",
    "    output_features = {\n",
    "        IMAGE_FEATURE_NAME: images,\n",
    "        JOINT_POSE_FEATURE_NAME: joint_poses,\n",
    "        ACTION_FEATURE_NAME: actions\n",
    "    }\n",
    "\n",
    "    return output_features, None\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_0hFfI8-xDNB"
   },
   "outputs": [],
   "source": [
    "def get_flags(dataset_type):\n",
    "  import prediction_train\n",
    "  \n",
    "  FLAGS = prediction_train.FLAGS\n",
    "  try:\n",
    "      tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  FLAGS.is_training = False\n",
    "  FLAGS.use_tpu = False\n",
    "  FLAGS.use_image_summary = False\n",
    "  FLAGS.dataset_type = dataset_type\n",
    "  FLAGS.use_legacy_vars = True\n",
    "  \n",
    "  if dataset_type == \"robot\":\n",
    "    FLAGS.data_dir=\"<Your download path>\"\n",
    "    FLAGS.sequence_length = 20\n",
    "    FLAGS.skip_num = 1\n",
    "    FLAGS.context_frames = 2\n",
    "    FLAGS.use_image_summary = False\n",
    "  else:\n",
    "    FLAGS.data_dir=\"gs://unsupervised-hierarch-video/data/\"\n",
    "    FLAGS.sequence_length = 256\n",
    "    FLAGS.skip_num = 2\n",
    "    FLAGS.context_frames = 5\n",
    "    FLAGS.use_image_summary = False\n",
    "    FLAGS.use_legacy_vars = False\n",
    "    \n",
    "  return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zQGwtGlnBK6J"
   },
   "outputs": [],
   "source": [
    "dataset_type = \"human\"\n",
    "\n",
    "def get_images(model_dir, flags, num_to_eval=100, pattern=\"humans-test\"):\n",
    "\n",
    "  run_config = tf.contrib.learn.RunConfig(\n",
    "    model_dir=set_model_dir,\n",
    "  )\n",
    "\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn=prediction_model.make_model_fn(flags), config=run_config)\n",
    "\n",
    "  predictions = estimator.predict(\n",
    "          input_fn=get_input_fn_queue_determ(pattern, 8, flags))\n",
    "    \n",
    "  num_evals = 0\n",
    "  van_out_psnr_all = []\n",
    "  van_on_enc_psnr_all = []\n",
    "  print(predictions)\n",
    "\n",
    "  all_runs = []\n",
    "\n",
    "  for prediction in predictions:\n",
    "    all_rows = {}\n",
    "    gt_images = prediction[\"gt_images\"] #[1:]\n",
    "    #van_on_enc = prediction[\"van_on_enc_all\"]\n",
    "    mask_out = prediction[\"mask_out_all\"]\n",
    "    van_out = prediction[\"van_out_all\"]\n",
    "    \n",
    "    gt_images_row = []\n",
    "    van_out_row = []\n",
    "    mask_out_row = []\n",
    "    for frame_i in range(len(van_out)):\n",
    "      van_out_row.append(van_out[frame_i])\n",
    "      mask_rgb = np.tile(mask_out[frame_i], [1, 1, 3])\n",
    "      mask_out_row.append(mask_rgb)\n",
    "    for frame_i in range(len(gt_images)):\n",
    "      gt_images_row.append(gt_images[frame_i])\n",
    "    all_rows[\"gt_images\"] = gt_images_row\n",
    "    all_rows[\"van_out\"]= van_out_row\n",
    "    all_rows[\"mask_out\"] = mask_out_row\n",
    "    #all_rows[\"van_on_enc\"]= van_on_enc\n",
    "\n",
    "    all_runs.append(all_rows)\n",
    "\n",
    "    num_evals += 1\n",
    "    if num_evals >= num_to_eval:\n",
    "      break\n",
    "      \n",
    "  del predictions\n",
    "      \n",
    "  return all_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hDWZ0Wqn0xra"
   },
   "outputs": [],
   "source": [
    "# Change this to your path to save the images.\n",
    "base_dir = \"/mnt/brain6/scratch/rubville/projects/unsupervised-hierarch-video-prediction/gen_frames/\"\n",
    "\n",
    "def save_imgs(images, folder, key=\"van_out\"):\n",
    "  for run_num in range(len(images)):\n",
    "    sys.stdout.flush()\n",
    "    frame_nums = range(len(images[run_num][key]))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    dir_path = os.path.join(folder, str(run_num))\n",
    "    if not os.path.exists(dir_path):\n",
    "      os.makedirs(dir_path)\n",
    "\n",
    "    for frame_i in frame_nums:     \n",
    "      frame = images[run_num][key][frame_i]\n",
    "      #frame = scipy.misc.imresize(frame, 4.0)\n",
    "      save_name = frame_i\n",
    "      if key == \"gt_images\":\n",
    "        # Make the number of the ground truth frames line up with the predicted frames.\n",
    "        save_name = frame_i - 1\n",
    "      save_png(frame,  os.path.join(dir_path, \"frame\"+str(save_name)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Foc_PnzDsAn6"
   },
   "outputs": [],
   "source": [
    "# Run to save results from EPVA Gan\n",
    "# This code will take a while to run since it hast to construct a large graph.\n",
    "# Decrease flags.sequence_length for a faster runtime.\n",
    "\n",
    "flags = get_flags(dataset_type)\n",
    "\n",
    "flags.enc_pred_use_l2norm = True\n",
    "flags.enc_size = 64\n",
    "flags.pred_noise_std = 1.0\n",
    "set_model_dir = \"gs://unsupervised-hierarch-video/pretrained_models/epva_wgan_human/\"\n",
    "# flags.sequence_length = 64 # Comment out to repo the results in the paper.\n",
    "all_runs_epva_wgan = get_images(set_model_dir, flags, num_to_eval=1000)\n",
    "save_imgs(all_runs_epva_wgan, os.path.join(base_dir, \"human_epva_wgan_frames\"), key=\"van_out\")\n",
    "save_imgs(all_runs_epva_wgan, os.path.join(base_dir, \"human_epva_wgan_masks\"), key=\"mask_out\")\n",
    "# Also saves the ground truth images.\n",
    "save_imgs(all_runs_epva_wgan, os.path.join(base_dir, \"human_gt\"), key=\"gt_images\")\n",
    "all_runs_epva_wgan = None\n",
    "del all_runs_epva_wgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "_r4obqf_bh65"
   },
   "outputs": [],
   "source": [
    "# Run to save results from EPVA\n",
    "\n",
    "flags = get_flags(dataset_type)\n",
    "\n",
    "flags.enc_pred_use_l2norm = False\n",
    "flags.enc_size = 64\n",
    "flags.pred_noise_std = 0\n",
    "flags.sequence_length = 64 # Comment out to repo the results in the paper.\n",
    "set_model_dir = \"gs://unsupervised-hierarch-video/pretrained_models/epva_human/\"\n",
    "all_runs_epva = get_images(set_model_dir, flags, num_to_eval=1000)\n",
    "save_imgs(all_runs_epva, os.path.join(base_dir, \"human_epva\"), key=\"van_out\")\n",
    "all_runs_epva = None\n",
    "del all_runs_epva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "lNuT1uFnWx03"
   },
   "outputs": [],
   "source": [
    "# Run to save results from E2E\n",
    "\n",
    "flags = get_flags(dataset_type)\n",
    "\n",
    "flags.enc_pred_use_l2norm = False\n",
    "flags.enc_size = 32\n",
    "flags.use_legacy_vars = True\n",
    "flags.sequence_length = 64 # Comment out to repo the results in the paper.\n",
    "set_model_dir = \"gs://unsupervised-hierarch-video/pretrained_models/e2e_human/\"\n",
    "all_runs_e2e = get_images(set_model_dir, flags, num_to_eval=1000)\n",
    "save_imgs(all_runs_e2e, os.path.join(base_dir, \"human_e2e\"), key=\"van_out\")\n",
    "all_runs_e2e = None\n",
    "del all_runs_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "generation.ipynb",
   "provenance": [
    {
     "file_id": "/piper/depot/google3/experimental/users/wichersn/generation2.ipynb?workspaceId=wichersn%3Agan_hierch%3A%3Acitc",
     "timestamp": 1526708932201
    },
    {
     "file_id": "1a18aZ1-UtCsSzPb5KdhSJKsJiH-ybWWa",
     "timestamp": 1526708783799
    },
    {
     "file_id": "16X4p7GKeiDLlCek8yJKuRXzYvT4x19vX",
     "timestamp": 1517608734577
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
